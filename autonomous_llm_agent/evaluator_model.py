# autonomous_llm_agent/evaluator_model.py

import asyncio # Added for async main test
import json
from llm_interface import call_llm # call_llm is now async
from config import EVALUATOR_MODEL_NAME, INITIAL_CONSTITUTION, DEFAULT_TEMPERATURE, DEFAULT_MAX_TOKENS

class Evaluator:
    def __init__(self, model_name: str = None):
        """
        Initializes the Evaluator model.
        """
        self.model_name = model_name if model_name else EVALUATOR_MODEL_NAME
        if not self.model_name:
            raise ValueError("Evaluator model name not specified and not found in config.")
        print(f"Evaluator initialized with model: {self.model_name}")

    def _format_constitution_for_prompt(self, constitution: dict) -> str:
        """Helper to format the constitution dictionary into a string for the prompt."""
        formatted_criteria = []
        for key, description in constitution.items():
            formatted_criteria.append(f"- Criterion '{key}': {description}")
        return "\n".join(formatted_criteria)

    def _parse_evaluation_response(self, evaluation_text: str) -> dict:
        """
        Parses the raw text response from the evaluator LLM to extract a verdict and feedback.
        """
        status = "unknown"
        feedback = evaluation_text 

        if "[[APPROVED]]" in evaluation_text:
            status = "approved"
        elif "[[REJECTED]]" in evaluation_text:
            status = "rejected"
        
        return {"status": status, "feedback": feedback.strip()}

    async def evaluate_response(self, task_prompt: str, worker_response: str, constitution: dict = None) -> dict | None: # Changed to async def
        """
        Evaluates a worker's response based on a given constitution asynchronously.
        """
        current_constitution = constitution if constitution else INITIAL_CONSTITUTION
        if not current_constitution:
            print("Error: No constitution provided for evaluation.")
            return {"status": "error", "feedback": "No constitution provided.", "raw_evaluation": ""}

        criteria_str = self._format_constitution_for_prompt(current_constitution)

        evaluator_prompt = f"""
You are an AI assistant serving as an impartial evaluator. Your task is to assess the quality of a response generated by another AI (Worker AI) based on a specific task and a set of evaluation criteria.

**Original Task Given to Worker AI:**
{task_prompt}

**Worker AI's Response:**
{worker_response}

**Evaluation Criteria (Constitution):**
{criteria_str}

**Your Evaluation Instructions:**
1.  Carefully review the Worker AI's response in the context of the Original Task and the Evaluation Criteria.
2.  For each criterion, implicitly consider if the response meets it.
3.  Provide overall feedback on the response, highlighting strengths and areas for improvement. Be specific.
4.  Conclude your evaluation with a clear verdict:
    - If the response is satisfactory and meets the core requirements of the task and criteria, include the tag "[[APPROVED]]" in your response.
    - If the response has significant issues, fails to meet key criteria, or needs substantial revision, include the tag "[[REJECTED]]" in your response.

Please structure your evaluation clearly.
"""
        print(f"Evaluator ({self.model_name}) evaluating response for task: \"{task_prompt[:100]}...\"")
        
        evaluation_raw_text = await call_llm( # Added await
            model_name=self.model_name,
            messages=[{"role": "user", "content": evaluator_prompt}],
            temperature=0.4, 
            max_tokens=DEFAULT_MAX_TOKENS
        )

        if not evaluation_raw_text:
            print(f"Evaluator ({self.model_name}) failed to get an evaluation response.")
            return {"status": "error", "feedback": "Failed to get evaluation from LLM.", "raw_evaluation": ""}

        print(f"Evaluator ({self.model_name}) received raw evaluation: \"{evaluation_raw_text[:100]}...\"")
        
        parsed_evaluation = self._parse_evaluation_response(evaluation_raw_text)
        parsed_evaluation["raw_evaluation"] = evaluation_raw_text

        return parsed_evaluation

async def test_evaluator_logic(): # New async function for tests
    print("Testing evaluator_model.py (async)...")
    evaluator = None # Initialize for broader scope

    try:
        evaluator = Evaluator()
    except ValueError as e:
        print(f"Error initializing evaluator: {e}")
        print("Please ensure EVALUATOR_MODEL_NAME is set in config.py or pass a model_name to Evaluator().")
    except Exception as e:
        print(f"An unexpected error occurred during Evaluator initialization: {e}")


    if evaluator:
        sample_task = "Describe the process of photosynthesis."
        sample_worker_response = "Plants use sunlight, water, and carbon dioxide to create glucose and oxygen. This happens in chloroplasts."

        print(f"\nEvaluating worker response for task: \"{sample_task}\"")
        print(f"Worker response: \"{sample_worker_response}\"")
        
        evaluation_result = await evaluator.evaluate_response(sample_task, sample_worker_response) # Added await
        
        if evaluation_result:
            print("\nEvaluator's Assessment:")
            print(f"  Status: {evaluation_result.get('status')}")
            print(f"  Feedback (excerpt): {evaluation_result.get('feedback', '')[:200]}...")
        else:
            print("\nEvaluator failed to provide an assessment.")
            
        print("\n--- Testing with a clearly deficient response (async) ---")
        deficient_worker_response = "Photosynthesis is when plants eat sunshine."
        print(f"Evaluating deficient worker response: \"{deficient_worker_response}\"")
        
        deficient_evaluation_result = await evaluator.evaluate_response(sample_task, deficient_worker_response) # Added await
        if deficient_evaluation_result:
            print("\nEvaluator's Assessment (for deficient response):")
            print(f"  Status: {deficient_evaluation_result.get('status')}")
            print(f"  Feedback (excerpt): {deficient_evaluation_result.get('feedback', '')[:200]}...")
        else:
            print("\nEvaluator failed to provide an assessment for the deficient response.")
    else:
        print("Evaluator could not be initialized due to errors. Skipping tests.")

if __name__ == '__main__':
    asyncio.run(test_evaluator_logic()) # Use asyncio.run()
